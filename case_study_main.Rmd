---
title: "Real Data Analysis"
author: "Anonymous"
output:
  html_document: default
  pdf_document: default
---

This Rmd is devoted to an empirical analysis of the same data set as that in Houtepen et al. (2016) and van Kesteren & Oberski (2019), for studying how DNA methylation plays a role in the regulation of human stress reactivity. This Rmd aims to reproduce the results in Section 3 of the paper "High-dimensional mediation analysis for selecting DNA methylation Loci mediating childhood trauma and cortisol stress reactivity".

```{r setup, include=FALSE}
library(knitr)
```


# 1. Preprocess raw data
Before running this Rmd, one should first download the data and specify the path of the data.

## 1.1 Download data and set up correct paths
- Step 1. Download data `E-GEOD-77445.processed.1.zip` (https://www.ebi.ac.uk/arrayexpress/files/E-GEOD-77445/E-GEOD-77445.processed.1.zip) as well as `E-GEOD-77445.processed.2.zip` 
(https://www.ebi.ac.uk/arrayexpress/files/E-GEOD-77445/E-GEOD-77445.processed.2.zip). 
         Unzip these two zip files in the `data` folder.
         
- Step 2. Run High-dimensional-mediation-analysis-JASA.Rproj by double clicking its filename to set up
          working directory 

- Step 3. Specify the `data` folder's absolute path as a string to the `PATH` variable below. Caution: the PATH should end with `/` or `\\`. For example, we store the unzipped files in following PATH:


```{r echo=TRUE, message=FALSE, warning=FALSE}
PATH = "E:/JASAACS20210115Code/data/"

# Load source codes
source("utils_mediation.R")
```

Note that there is no need to download `E-GEOD-77445.sdrf.txt`, which has different row length and cannot be read into R directly. We have downloaded and manually cleaned it. We saved it as `X.tsv` in the `data` folder because this file contains information about exposure variable.

- Step 3. After finishing Step 1 & 2, you can click `Knit` (in the upper left corner) to run the entire markdown.

## 1.2 Load data
We loaded these original .txt files and merged them into a clean data set (containing two vectors, exposure variable X as well as response variable y, and two dataframes potential mediators M as well as confounding variables S). To make it convenient for further analysis, we stored the data set in a .RData file.

The following code implements merging the raw data:
```{r RawData, echo=TRUE, warning=FALSE}
RawDataPath = LoadRawData(InputPath = PATH)
```

## 1.3 Marginal screening for potential mediators
We  carry out a screening step to
retain the top 1000 potential mediators by ranking the absolute value of the
product of two correlations - the correlation between $\mathbf{x}$ and each element of $\mathbf{m}$, and between $y$ and each element of $\mathbf{m}$. This indeed is a marginal screening procedure based on Pearson correlation proposed by Fan & Lv (2008).

The following codes are to show marginal screening step
```{r Preprocess, echo=TRUE, message=TRUE, warning=FALSE, paged.print=FALSE}
topPotential = 1000 #retain the top 1000 potential mediators
cleanedData = MarginalScreening(RawDataPath, topPotential = topPotential)
```

Now we are ready to load the cleaned data:
```{r}
load(cleanedData)
```


# 2. Our proposed new method
Procedure of identifying active mediators in the mediation
models (2.1) and (2.2), and estimating the direct effect $\mathbf{\alpha_1}$ and indirect effect $\boldsymbol{\beta}$ that can get around high dimensional matrix estimation.

## 2.1 Identifying active mediators
we apply the partial penalized least squared method to fit model (2.1) by only penalizing $\boldsymbol{\alpha}_0$, which corresponding to Eq. (2.5). 

### 2.1.1 Implement the algorithm for solving Eq. (2.5)
We apply the local linear approximation algorithm
(LLA) in (Zou & Li, 2008) with the SCAD penalty (Fan & Li, 2001), and set $a=3.7$.
$$
p'_{\lambda}(t)=\lambda\{I(t\leq \lambda)
+\frac{(a\lambda-t)_{+}}{(a-1)\lambda}I(t>\lambda)\},
$$
Below is the function of the first order derivative of SCAD penalty
```{r SCAD, echo=TRUE, warning=FALSE, paged.print=TRUE}
deSCAD <- function(z,lamb,a=3.7){
  # First order derivative of SCAD penalty
  # tuning parameter "a" (or "gamma") use the default value 3.7
  return(1*(z<=lamb)+pmax((a*lamb-z),0)/((a-1)*lamb)*(lamb<z))
}
```

The numerical algorithm to solve Eq. (2.5), the partial penalized least squares problem, is given in Section S.4 in the supplementary material of this paper.

Below is the implementation of the local linear approximation Zou and Li (2008). Note that in R package `glmnet`, by passing the weight $w$ to argument `penalty.factor`, we can assign different shrinkage to each coefficient (see details in `help(glmnet)`). 
```{r LLA, echo=TRUE, warning=FALSE, paged.print=TRUE}
## ONE-STEP SPARSE ESTIMATES IN NONCONCAVE PENALIZED LIKELIHOOD MODELS
LLA_h1<- function(X,Y,M,lamb,n,p,q, n_imp = 0,S = NULL){
  #' Local linear approximation algorithm
  #' @param X matrix or dataframe, n by q exposure matrix. q can be 1, and requires q < n
  #' @param Y vector, n-dimensional outcome vector.
  #' @param M matrix or dataframe, n by p mediator matrix and q < n.
  #' @param lamb float, tuning parameter lambda for the penalty
  #' @param n int, the sample size
  #' @param p int, X matrix dimension (number of columns) 
  #' @param q int, M matrix dimension (number of columns)
  #' @param n_imp int, for important mediators that will not be penalized

  if(length(S) == 0){
    s = 0
    V = X
  }else{
    s = ncol(S)
    V = cbind(X,S)
  }
  # Step 1 using Lasso
  w = matrix(0,nrow = (p + q + s),ncol=1)
  w[1:(p-n_imp)] = 1
  alpha_int = coef(glmnet(cbind(M,V),Y,family = 'gaussian', alpha=1,
                          lambda = lamb, penalty.factor=w,intercept = FALSE))[-1]
  # Step 2 using local linear approximation of SCAD
  w = matrix(0,nrow = (p+q + s),ncol=1)
  for(j in 1:(p- n_imp)){
    w[j] = deSCAD(alpha_int[j],lamb)
  }
  alpha = coef(glmnet(cbind(M,V),Y,family = 'gaussian', alpha=1,
                      lambda = lamb, penalty.factor=w, intercept = FALSE))
  return(alpha[-1])
}
```

The tuning parameter $\lambda$ for our method is chosen based on the high-dimensional BIC (HBIC) method in Wang et al. (2013). The following function is to calculate HBIC for a given tuning parameter $\lambda$
```{r HBIC, echo=TRUE, warning=FALSE, paged.print=TRUE}
HBIC_calc <- function(lamb, xx,yy,mm,S = NULL, n_imp =8){
  #‘ Calculate HBIC for a specific tuning parameter lambda
  #' @param lamb float, tuning parameter lambda for penality function
  #' @param xx n by q exposure matrix. q can be 1, and q < n is required
  #' @param yy n-dimensional outcome vector.
  #' @param mm n by p mediator matrix. p can be larger than n.
  #' @param S n by s confounding variable matrix. s can be 1, and s < n is required.
  #' @param n_imp int, number of important mediators that will not be penalized
  
  #' @return
  #'        A list of class `HBIC_calc'
  #'        - BIC: HBIC score
  #'        - alpha0: estimated alpha0,
  #'        - alpha1: estimated alpha1,
  #'        - alpha2: estimated alpha2,
  #'        - sigma1_hat: estimated sigma1
  #'   
  n = nrow(xx)
  p = ncol(mm)
  q = ncol(xx)
  if(is.null(S)){
    s = 0
    result <- LLA_h1(xx,yy,mm,lamb,n,p,q, n_imp = n_imp)
    alpha0 = result[1:p]
    alpha1 = result[(p+1):(p+q)]
    alpha2 = NULL
    tmp = yy - mm%*%alpha0 - xx%*% alpha1
  }else{
    s = ncol(S)
    result <- LLA_h1(xx,yy,mm,lamb,n,p,q,n_imp = n_imp, S = S)
    alpha0 = result[1:p]
    alpha1 = result[(p+1):(p+q)]
    alpha2 = result[(p+q+1):(p+q+s)]
    
    tmp = yy - mm%*%alpha0 - xx%*% alpha1 - S %*% alpha2
  }
  
  df = length(which(alpha0!= 0))+q + s
  sigma_hat = t(tmp)%*%tmp/n
  BIC = log(sigma_hat) + df*log(log(n))*log(p+q + s)/n
  #obj = objective(xx,yy,M,alpha0,alpha1,lamb)
  return(list(BIC=BIC,alpha0=alpha0,alpha1 = alpha1, 
              alpha2 = alpha2, sigma1_hat = sigma_hat))
}

```

The following function integrates the LLA algorithm with HBIC parameter tuning. It will choose the $\lambda$ from given grid points `lam_list` that yields the smallest HBIC value and return the selected mediators.
```{r warning=FALSE}
HBIC_PPLS <- function(X, y, M, S = NULL, lam_list =NULL, n_imp =8, topPotential =1000){
  #' This function implements solving the partially penalized least squares with the 
  #' SCAD penalty. The tuning parameter lambda for the penalty function is chosen
  #' based on the high-dimensional BIC (HBIC) method.
  
  #' @param X The n by q exposure matrix. q can be 1, and q < n is required
  #' @param y The n-dimensional outcome vector.
  #' @param M The n by p mediator matrix. p can be larger than n.
  #' @param S The n by s confounding variables matrix. s can be 1, and s < n is required. 
  #' @param lam_list a list of tuning parameter for HBIC
  #' @param n_imp int, for important mediators that will not be penalized

  
  #' @return
  #'
  #'    A dataframe of selected important mediators (denoted as M_{\hat{A}} in the paper)
  hbic= c()
  
  result =lapply(lam_list, HBIC_calc, xx=X,yy=y, mm=M, S = S, n_imp = n_imp)
  
  for( ii in 1: length(lam_list)){
    hbic[ii] = result[[ii]]$BIC
  }
  # Find minimum HBIC score's corresponding lambda
  id = which(hbic==min(hbic))
  id = tail(id,1)
  lamb = lam_list[id]
  print(paste("choose lambda:", lamb))
  result = result[[id]]
  alpha0_hat = result$alpha0
  alpha1_hat = result$alpha1
  
  A = which(alpha0_hat!=0)
  # Selected mediators 
  M_A = M[,c(A[A>topPotential],A[A<=topPotential])]
  return(M_A)
}
```

The candidate set of $\lambda$ is 50 equally spaced grid from 20 to 70. Below are the codes to specify grid points for $\lambda$ and then use the `HBIC_PPLS` function to select important mediators:

```{r warning=FALSE}
# specify the candidate set of lambda
lamb_min = 20
lamb_max = 70
ngrid = 50
lam_list = seq(lamb_min,lamb_max,length.out = ngrid)

M_A = HBIC_PPLS(X, y, M, S = S, lam_list =lam_list, n_imp =8, 
                topPotential = topPotential)
```

### 2.1.2 Estimate $\mathbf{\alpha}_1$ and $\mathbf{\alpha}_0$ 
Estimated Coefficients, SE, t-values and p-values (Table 1)
```{r Table2, warning=FALSE}
colnames(S) = paste0("Z",0:(ncol(S)-1))
df = data.frame(cbind(M_A, X, S))
round(summary(lm(y~ 0+., data = df))$coefficients,digits = 3)
```


### 2.1.3 Estimate $\Gamma_1$ and $\Gamma_2$ 
Estimated Coefficients of $\Gamma_1$ and $\Gamma_2$ and their p-values (Table 2)
```{r warning=FALSE}
cpg = colnames(M_A)
colnames(M_A) = paste0("m",1:ncol(M_A))
Gamma = MediatorExposure(cbind(X, S), M_A)
round(Gamma$Gamma_hat,digits = 3)
round(Gamma$Gamma_pvalue,digits = 3)
```


## 2.2 Test of direct effect and indirect effect 
The estimated coefficients, standard errors, test statistics values and p-values (Table 3)
```{r paged.print=FALSE}
direct_indirect_eff <- mediationInference(X, y, M_A, S)
knitr::kable(direct_indirect_eff$summary_result)
```



## 2.3 Wrapper function
The above are the break down of our proposed method so that users can follow the analysis step by step. We also provide a wrapper function `hdMediation` that consolidate the estimation and inference steps (section 2.1 ~ 2.2). Calling this function will return selected important mediators' name, estimated direct as well as indirect effect, test statistics and p-value. Note that users should put the unpenalized mediators in the last part of $M$ matrix and use `n_imp` to specify the number of them.
```{r}
fit<-hdMediation(X,y,M, lam_list =lam_list, S=S, n_imp =8)
# Inportant mediators:
print(fit$Mediators_imp)
knitr::kable(fit$summary_result)
```

## 2.4 Annotation 
Annotation of the included mediators (Table 4)
```{r annotation, message=FALSE, warning=FALSE}
hm450 <- get450k()
probes <- hm450[cpg]
annot <- getNearestTSS(probes)
annot[,'nearestGeneSymbol', drop=FALSE]
```

# 3. Some comparisons
Compare our results with those in Houtepen et al. (2016) and van Kesteren & Oberski (2019) from statistical point of view.

Below are the codes to produce the estimated $\alpha_j$’s and their SE and p-values in  model $\mathbf{m}_{(1)}$, $\mathbf{m}_{(2)}$, and $\mathbf{m}_{(3)}$. (Table 5)
```{r Table5, warning=FALSE}
## m_{(1)}  Houtepen et al (2016)
Houtepen<-analysis_helper(X,y,M_A[,1:3],
                          S, mod_name = '$\\mathbf{m}_{(1)}$')

##m_{(2)} van Kesteren & Oberski (2019)
Kesteren<-analysis_helper(X,y,M_A[,4:8], S, mod_name = '$\\mathbf{m}_{(2)}$')

## m_{(3)} Mediators merge from Houtepen et al (2016) or van Kesteren & Oberski (2019)
combine8<-analysis_helper(X,y,M_A[,1:8], S,  mod_name = '$\\mathbf{m}_{(3)}$')

Table5 <- merge(Houtepen$mod_summary, 
                Kesteren$mod_summary,by = 'row.names', 
                all = TRUE, check.names=F)
Table5 <- merge(Table5, combine8$mod_summary,
                by.x = 'Row.names', by.y ='row.names' , 
                all = TRUE, check.names=F)
Table5[is.na(Table5)] <- ""
colnames(Table5)[1] <- " "
knitr::kable(Table5)
```

The estimated coefficients, standard errors, test statistics values and p-values for model $\mathbf{m}_{(1)}$, $\mathbf{m}_{(2)}$, $\mathbf{m}_{(3)}$ (Table 6)

```{r Table6, warning=FALSE}
Table6 <- rbind(Houtepen$summary_result, Kesteren$summary_result, combine8$summary_result)
row.names(Table6) <- c('$\\mathbf{m}_{(1)}$', '', '$\\mathbf{m}_{(2)}$' , 
                       ' ', '$\\mathbf{m}_{(3)}$', '  ')
knitr::kable(Table6)
```

# 4. Relationship among the mediators
## 4.1 Sample pearson correlation
The following code shows the sample Pearson correlation $\hat{\rho}(m_j, m_k)$ and its $p$-values for $H_0: \rho(m_j, m_k)=0$ (Table S.2)
The lower triangle of the table lists their pairwise Pearson correlations, and the upper triangle provides the corresponding
p-values for testing the pairwise correlations.
```{r samplePearson, warning=FALSE}
knitr::kable(Fisher.helper(M_A))
```


## 4.2 Sample partial correlation
The following codes present the sample partial correlation $\hat{\rho}(m_j, m_k|X,\mathbf{z})$ and its $p$-values for $H_0: \rho(m_j, m_k|X,\mathbf{z})=0$ (Table S.3)

```{r partialPearson, warning=FALSE}
df = data.frame(cbind(M_A, X, S))
par.corr.x = partial.helper(df, 1:11, 12:21 )
knitr::kable(par.corr.x)
```

## 4.3 Multi-collinearity between mediators
The $R^2$, $F$ statistics values and $p$-values of regression models between mediators to investigate multi-collinearity between mediators. (Table S.4)
```{r multicollinearity, warning=FALSE}
fit1 <- summary(lm( m1~ m4+m5+m6+m7+m8, data = df))

fit2 <- summary(lm( m1~ m9+m10+m11, data = df))

fit3 <- summary(lm( m4~ m5+m6+m7+m8, data = df))

fit4 <- summary(lm( m5~ m4+m6+m7+m8, data = df))

fit5 <- summary(lm( m6~ m4+m5+m7+m8, data = df))

S4 <- generateTableS4(list(fit1 = fit1, fit2 = fit2,
                           fit3 = fit3,fit4 = fit4,fit5 = fit5))
tableS4<- data.frame(Dependent_variable = c('m1', 'm1', 'm4','m5','m6'),
                      Independent_variable = c('m4, m5, m6, m7, m8', 
                                               'm9, m10, m11',
                                               'm5, m6, m7, m8', 
                                               'm4, m6, m7, m8', 
                                               'm4, m5, m7, m8'),
                      R2 = S4$R2,
                      F_stat=S4$F_stat,
                      p_value = S4$p_val)
knitr::kable(tableS4, digits  = 4)
```

# References

Fan, J. & Li, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of American Statistical Association_, **96**, 1348-1360.

Fan, J. & Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, **70**(5), 849–911.

Houtepen, L.C., Vinkers, C.H., Carrillo-Roa, T., Hiemstra, M., Van Lier, P.A.,
Meeus, W., Branje, S., Heim, C.M., Nemeroff, C.B., Mill, J. & Schalkwyk,
L.C. (2016). Genome-wide DNA methylation levels and altered cortisol stress reactivity following childhood trauma in humans. _Nature Communications_, **7**(1), 10967

van Kesteren, E. J. & Oberski, D. L. (2019). Exploratory Mediation Analysis with
Many Potential Mediators _Structural Equation Modeling: A Multidisciplinary Journal_, **26**(5), 710-723.

Wang, L., Kim, Y. & Li, R.(2013). Calibrating non-convex penalized regression in ultra-high dimension. _Annals of Statistics_ **41**, 2505-2536

ZOU, H., & LI, R. (2008). One-step sparse estimates in nonconcave penalized likelihood models. _Annals of Statistics_ **36**(4), 1509-1533.
35
